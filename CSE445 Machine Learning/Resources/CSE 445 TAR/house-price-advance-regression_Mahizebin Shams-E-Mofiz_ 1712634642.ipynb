{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve, RandomizedSearchCV, KFold, cross_validate\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, Lasso, LassoCV, LassoLarsIC, Ridge, RidgeCV, TweedieRegressor, BayesianRidge, LinearRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew, boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "\n",
    "# Setting plot styling.\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data <a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "full_data = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "print('train_df\\t{}'.format(train_df.shape))\n",
    "print('test_df \\t{}'.format(test_df.shape))\n",
    "print('full_data \\t{}'.format(full_data.shape))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different columns between train and test columns\n",
    "set(train_df.columns) - set(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Id'\n",
    "train_df.drop(columns = 'Id', inplace=True)\n",
    "test_df.drop(columns = 'Id', inplace=True)\n",
    "full_data.drop(columns = 'Id', inplace=True)\n",
    "print('Drop column Id completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data <a id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis \n",
    "First we have to understand overall data by take a quick look and visualize them easy to gain information.\n",
    "\n",
    "**Check corralation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,9))\n",
    "\n",
    "corrmat = train_df.corr()\n",
    "sns.heatmap(corrmat, mask = corrmat < 0.75, linewidth = 0.5, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the correlated variables which are linear relationship highly that might be multicollinearity.\n",
    "\n",
    "Multicollinearity is a correlated with another independent variable in some case two or more other independent variables so that can reduce the performance of some algorithm. Estimates for regression coefficients can be unreliable and tests of significance for regression coefficients can be misleading.\n",
    "\n",
    "As figure above there are some variable are correlate:\n",
    "* `GarageYrBlt` and `YearBuilt`\n",
    "* `TotRmsAbvGrd` and `GrLivArea`\n",
    "* `1stFlrSF` and `TotalBsmtSF`\n",
    "* `GarageArea` and `GarageCars`\n",
    "\n",
    "**We would drop some of these variables later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr = abs(corrmat.SalePrice).sort_values(ascending=False).head(10)\n",
    "top_corr_col = list(top_corr.index)\n",
    "top_corr_col.remove('SalePrice')\n",
    "top_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would focus on the variables that are high correlation to **target variable** and then consider to eliminate outliers because they are values that are notably different from other data points, and they can cause problems like miss significant findings or distort real results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,9))\n",
    "\n",
    "corrmat = train_df[top_corr_col].corr()\n",
    "sns.heatmap(corrmat, linewidth = 0.5, cmap = 'Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Plot variable vs target variable(SalePrice)\n",
    "def plot_(df, col):\n",
    "    if df[col].dtype != 'object':\n",
    "        if len(df[col].unique()) <= 12:\n",
    "            fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "            sns.stripplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n",
    "            sns.countplot(df[col], ax=ax[1])\n",
    "            fig.suptitle(str(col) + ' analysis')\n",
    "        else:\n",
    "            fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "            sns.scatterplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n",
    "            sns.distplot(df[col], kde=False, ax=ax[1])\n",
    "            fig.suptitle(str(col) + ' analysis')\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1,2,figsize=(12,6), sharey=True)\n",
    "        sns.stripplot(x=col, y='SalePrice', alpha = 0.5, data=df, ax=ax[0])\n",
    "        sns.boxplot(x=col, y='SalePrice', data=df, ax=ax[1])\n",
    "        fig.suptitle(str(col) + ' analysis')\n",
    "        \n",
    "# Explore missing data\n",
    "def plot_missing(df):\n",
    "    miss_col = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending = False)\n",
    "    percent_miss = round((miss_col / len(df) *100),2)\n",
    "    missing = pd.DataFrame([miss_col, percent_miss]).T.rename(columns = {0:'Feature', 1:'% missing'})\n",
    "    return missing\n",
    "\n",
    "# Sort category vs target variable\n",
    "def sort_cate(df, col, target=\"SalePrice\"):\n",
    "    \n",
    "    \n",
    "    print(\"{}  | type: {}\\n\".format(col, df[col].dtype))\n",
    "    d = pd.DataFrame({\"n\": df[col].value_counts(),\n",
    "                                \"Ratio\": 100 * df[col].value_counts() / len(df),\n",
    "                                \"TARGET_MEDIAN\": df.groupby(col)[target].median(),\n",
    "                                \"Target_MEAN\": df.groupby(col)[target].mean()}).sort_values(by='TARGET_MEDIAN', ascending=False)\n",
    "    print(d, end=\"\\n\\n\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,6), sharey=True)\n",
    "    sns.stripplot(x=col, y='SalePrice', data=df, alpha = 0.5, order = d.index,  ax=ax[0])\n",
    "\n",
    "    sns.boxplot(x=col, y='SalePrice', data=df, order = d.index, ax=ax[1])\n",
    "    fig.suptitle(str(col) + ' analysis')\n",
    "    \n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     set_xticklabels(chart.get_xticklabels(ax=ax[1]), rotation=45, horizontalalignment='right')\n",
    "    plt.show();\n",
    "\n",
    "print('Plot functions are ready to use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cate(train_df, 'Neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in top_corr_col:\n",
    "#     plot_(train_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Outlierss <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some data points that are far from other data points. We would remove outliers which are problematic for many statistical analyses because they can distort real results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(train_df[(train_df['OverallQual'] < 5) & (train_df['SalePrice'] > 200000)].index)\n",
    "train_df = train_df.drop(train_df[(train_df['OverallQual'] > 9) & (train_df['SalePrice'] < 200000)].index)\n",
    "\n",
    "\n",
    "train_df = train_df.drop(train_df[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 200000)].index)\n",
    "\n",
    "train_df = train_df.drop(train_df[(train_df['GarageArea'] > 1200) & (train_df['SalePrice'] < 200000)].index)\n",
    "\n",
    "train_df = train_df.drop(train_df[(train_df['TotalBsmtSF'] > 3000) & (train_df['SalePrice'] < 200000)].index)\n",
    "\n",
    "train_df = train_df.drop(train_df[(train_df['1stFlrSF'] > 3000) & (train_df['SalePrice'] < 200000)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Variable (SalePrice)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 4))\n",
    "\n",
    "ax1.set_title('Distplot')\n",
    "sns.distplot(train_df['SalePrice'], fit=norm,  ax = ax1)\n",
    "\n",
    "ax2.set_title('Boxplot')\n",
    "sns.boxplot(train_df['SalePrice'], ax = ax2)\n",
    "\n",
    "\n",
    "print ('Skewness: ', np.round(train_df['SalePrice'].skew(), 2))\n",
    "print ('Kurtosis: ', np.round(train_df['SalePrice'].kurt(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target variable is a positive skewness, so we need to transform data by Log-transformation of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log(1+x) in order to transform target variable\n",
    "train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "ax1.set_title('Distplot Log-transformation')\n",
    "sns.distplot(train_df['SalePrice'],fit=norm,  ax = ax[0])\n",
    "\n",
    "ax2.set_title('Boxplot Log-transformation')\n",
    "sns.boxplot(train_df['SalePrice'], ax = ax[1])\n",
    "\n",
    "print ('Skewness: ', np.round(train_df['SalePrice'].skew(), 2))\n",
    "print ('Kurtosis: ', np.round(train_df['SalePrice'].kurt(), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value <a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PoolQC`: Pool quality\n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       NA\tNo Pool\n",
    "\n",
    "`MiscFeature`: Miscellaneous feature not covered in other categories\n",
    "\t\t\n",
    "       Elev\tElevator\n",
    "       Gar2\t2nd Garage (if not described in garage section)\n",
    "       Othr\tOther\n",
    "       Shed\tShed (over 100 SF)\n",
    "       TenC\tTennis Court\n",
    "       NA\tNone\n",
    "\n",
    "`Alley`: Type of alley access to property\n",
    "\n",
    "       Grvl\tGravel\n",
    "       Pave\tPaved\n",
    "       NA \tNo alley access\n",
    "       \n",
    "`BsmtQual`: Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\t\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "       \n",
    "For some feature such as `PoolQC`, `MiscFeature` and `Alley` there are moer than 90% of missing value but in this case NaN mean **No Pool**, **No Miscellaneous** and **No alley access**.\n",
    "\n",
    "We will replace NaN with '**None**' and for the other features we would take into account and impute NaN to the following column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none_cols = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n",
    "             'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n",
    "             'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n",
    "\n",
    "# Impute missing value\n",
    "for col in none_cols:\n",
    "    train_df[col].replace(np.nan, 'None', inplace=True)\n",
    "    test_df[col].replace(np.nan, 'None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other features such as `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath` all of these are numerical features if **No basement** that should be replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm=['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath','BsmtQual']\n",
    "train_df[bsm].groupby('BsmtQual').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like if **No Garage** `GarageYrBlt`, `GarageArea`, `GarageCars` should be replaced with 0 also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar = ['GarageYrBlt', 'GarageArea', 'GarageCars','GarageQual']\n",
    "train_df[gar].groupby('GarageQual').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these features by 0 \n",
    "zero_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']\n",
    "\n",
    "for col in zero_cols:\n",
    "    train_df[col].replace(np.nan, 0, inplace=True)\n",
    "    test_df[col].replace(np.nan, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these features by most frequency value or mode\n",
    "most_freq_cols = ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual','SaleType', 'Utilities']\n",
    "\n",
    "for col in most_freq_cols:\n",
    "    train_df[col].replace(np.nan, train_df[col].mode()[0], inplace=True)\n",
    "    test_df[col].replace(np.nan, test_df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MSZoning` should be considered to replace NaN by gruop of `MSSubClass` \n",
    "\n",
    "`LotFrontage` relate to `Neighborhood` so we will replace NaN by gruop of `Neighborhood`\n",
    "\n",
    "For `MSSubClass`, `YrSold`, `MoSold` should be change to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (10, 8))\n",
    "sns.countplot(x=\"MSSubClass\", hue=\"MSZoning\", data=train_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MSZoning'] = train_df.groupby('MSSubClass')['MSZoning'].apply(\n",
    "    lambda x: x.fillna(x.mode()[0]))\n",
    "test_df['MSZoning'] = test_df.groupby('MSSubClass')['MSZoning'].apply(\n",
    "    lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "\n",
    "train_df[\"LotFrontage\"] = train_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "test_df[\"LotFrontage\"] = test_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "\n",
    "# Change type to str\n",
    "chg_type = ['MSSubClass', 'YrSold', 'MoSold']\n",
    "train_df[chg_type] = train_df[chg_type].astype(str)\n",
    "test_df[chg_type] = test_df[chg_type].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cate(train_df, 'Neighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical to ordinal features\n",
    "def cate_to_ordinal(df):\n",
    "    \n",
    "    n_map = {'MeadowV': 1,'IDOTRR': 1,'BrDale': 1,'OldTown': 2,'Edwards': 2,'BrkSide': 2,\n",
    "                 'Sawyer': 3,'Blueste': 3,'SWISU': 3,'NPkVill': 3,'NAmes': 3,'Mitchel': 4,\n",
    "                 'SawyerW': 5,'NWAmes': 5,'Gilbert': 5,'Blmngtn': 5,'CollgCr': 5,\n",
    "                 'ClearCr': 6,'Crawfor': 6,'Veenker': 7,'Somerst': 7,'Timber': 7,\n",
    "                 'StoneBr': 8,'NoRidge': 9,'NridgHt': 10}\n",
    "    df['Neighborhood'] = df['Neighborhood'].map(n_map).astype('int')\n",
    "\n",
    "    ext_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "    df['ExterQual'] = df['ExterQual'].map(ext_map).astype('int')\n",
    "\n",
    "    ext_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "    df['ExterCond'] = df['ExterCond'].map(ext_map).astype('int')\n",
    "\n",
    "    bsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "    df['BsmtQual'] = df['BsmtQual'].map(bsm_map).astype('int')\n",
    "    df['BsmtCond'] = df['BsmtCond'].map(bsm_map).astype('int')\n",
    "\n",
    "    bsmf_map = {'None': 0,'Unf': 1,'LwQ': 2,'Rec': 3,'BLQ': 4,'ALQ': 5,'GLQ': 6}\n",
    "    df['BsmtFinType1'] = df['BsmtFinType1'].map(bsmf_map).astype('int')\n",
    "    df['BsmtFinType2'] = df['BsmtFinType2'].map(bsmf_map).astype('int')\n",
    "\n",
    "    heat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "    df['HeatingQC'] = df['HeatingQC'].map(heat_map).astype('int')\n",
    "    df['KitchenQual'] = df['KitchenQual'].map(heat_map).astype('int')\n",
    "    df['FireplaceQu'] = df['FireplaceQu'].map(bsm_map).astype('int')\n",
    "    df['GarageCond'] = df['GarageCond'].map(bsm_map).astype('int')\n",
    "    df['GarageQual'] = df['GarageQual'].map(bsm_map).astype('int')\n",
    "    \n",
    "cate_to_ordinal(train_df)\n",
    "cate_to_ordinal(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would create useful new features from existing feature in order to add more information to target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['TotalSF'] = train_df['BsmtFinSF1'] + train_df['BsmtFinSF2'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "\n",
    "test_df['TotalSF'] = test_df['BsmtFinSF1'] + test_df['BsmtFinSF2'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values of a certain independent variable (feature) are skewed, depending on the model, skewness may violate model assumptions (e.g. logistic regression) or may impair the interpretation of feature importance.\n",
    "In this case we will use Boxcox-Transformation to transform high skewness features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = list(train_df.select_dtypes(exclude='object').columns)\n",
    "num_col.remove('SalePrice')\n",
    "\n",
    "# Get skewness of numerical feature\n",
    "skew_feature = abs(train_df[num_col].apply(lambda x: skew(x))).sort_values(ascending = False)\n",
    "\n",
    "# Filter feature that has skewness > 0.75 \n",
    "high_skew = skew_feature[skew_feature > 0.75]\n",
    "\n",
    "# Transform by using Boxcox-Transformation\n",
    "\n",
    "for f in high_skew.index:\n",
    "    train_df[f] = boxcox1p(train_df[f], boxcox_normmax(train_df[f] + 1))\n",
    "    test_df[f] = boxcox1p(test_df[f], boxcox_normmax(test_df[f] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Utilities'].value_counts(normalize = True).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thd = 0.95\n",
    "drop_col = []\n",
    "\n",
    "for column in train_df.drop('SalePrice', axis = 1):\n",
    "    \n",
    "    most_freq_value = train_df[column].value_counts(normalize = True).iloc[0]\n",
    "    if most_freq_value > thd:\n",
    "        drop_col.append(column)\n",
    "        print ('{}: \\t {}% same value'.format(column, np.round(most_freq_value, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col += ['GarageYrBlt','TotRmsAbvGrd', '1stFlrSF', 'GarageCars']\n",
    "# Drop column\n",
    "train_df.drop(columns = drop_col, inplace=True)\n",
    "test_df.drop(columns = drop_col, inplace=True)\n",
    "\n",
    "print ('Drop Columns Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['SalePrice']\n",
    "train_X = train_df.drop(columns = 'SalePrice')\n",
    "\n",
    "print ('Splitting \"SalePrice\" Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Get categorical columns\n",
    "cat_col = train_X.select_dtypes(include='object').columns\n",
    "num_col = train_X.select_dtypes(exclude='object').columns\n",
    "\n",
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_cols_train = pd.DataFrame(enc.fit_transform(train_X[cat_col]))\n",
    "OH_cols_test = pd.DataFrame(enc.transform(test_df[cat_col]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = train_X.index\n",
    "OH_cols_test.index = test_df.index\n",
    "\n",
    "# Put columns name into One-hot encoding columns\n",
    "OH_cols_train.columns = enc.get_feature_names(cat_col)\n",
    "OH_cols_test.columns = enc.get_feature_names(cat_col)\n",
    "\n",
    "# Concat Categorical with Numerical columns\n",
    "X_train = pd.concat([train_X[num_col], OH_cols_train], axis=1)\n",
    "X_test = pd.concat([test_df[num_col], OH_cols_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling <a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LinearRegression\n",
    "\n",
    "print('='*30 +'BASE MODEL' + '='*30)\n",
    "slr = LinearRegression()\n",
    "slr.fit(X_train, y_train)\n",
    "print('slope ： {:0.2f}'.format(slr.coef_[0]))\n",
    "print('intercept : {:0.2f}'.format(slr.intercept_))\n",
    "print('\\n')\n",
    "y_pre = slr.predict(X_train)\n",
    "y_test = slr.predict(X_test)\n",
    "RMSE = -1*cross_val_score(slr, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean()\n",
    "print(f'RMSE: {RMSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('LinearRegression', LinearRegression()),\n",
    "          (\"Ridge\", Ridge()),\n",
    "          (\"Lasso\", Lasso()),\n",
    "          (\"ElasticNet\", ElasticNet()),\n",
    "          ('RandomForestRegressor', RandomForestRegressor()),\n",
    "          ('GradientBoostingRegressor', GradientBoostingRegressor()),\n",
    "          (\"XGBoost\", XGBRegressor()),\n",
    "          (\"LightGBM\", LGBMRegressor()),\n",
    "          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n",
    "\n",
    "print('='*30 +'RMSE BASE MODEL' + '='*30)\n",
    "\n",
    "mean_rmse = []\n",
    "sd_rmse = []\n",
    "model_name = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    RMSE = -1*cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    print(f'{name:<25} {RMSE.mean():.5f} ±{RMSE.std():.3f}')\n",
    "    \n",
    "    mean_rmse.append(RMSE.mean())\n",
    "    sd_rmse.append(RMSE.std())\n",
    "    model_name.append(name)\n",
    "\n",
    "    \n",
    "base = pd.DataFrame({'model':model_name, 'mean_rmse':mean_rmse, 'sd_rmse':sd_rmse})\n",
    "sns.barplot(x=\"mean_rmse\", y=\"model\", data=base, orient = 'h',**{'xerr': sd_rmse})\n",
    "plt.title('Baseline: Cross Validation Scores')\n",
    "plt.axvline(x = np.mean(mean_rmse), color = 'firebrick', linestyle = '--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning <a id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "pipe = make_pipeline(RobustScaler(), Ridge())\n",
    "\n",
    "param_grid = {'ridge__alpha': [0.01, 0.1, 1, 10, 15 ,20, 25, 30]}\n",
    "\n",
    "grid_rid = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n",
    "\n",
    "best_grid_rid = grid_rid.fit(X_train, y_train)\n",
    "\n",
    "rid_param = best_grid_rid.best_params_\n",
    "\n",
    "print(f'best_params_: {rid_param}')\n",
    "print(f'score: {-1*best_grid_rid.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "pipe = make_pipeline(Lasso())\n",
    "\n",
    "param_grid = {'lasso__alpha': [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008],\n",
    "              'lasso__max_iter': [1e3, 1e5, 1e7]}\n",
    "\n",
    "grid_las = GridSearchCV(pipe, param_grid = param_grid, cv = 5, scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n",
    "\n",
    "best_grid_las = grid_las.fit(X_train, y_train)\n",
    "\n",
    "las_param = best_grid_las.best_params_\n",
    "\n",
    "print(f'best_params_: {las_param}')\n",
    "print(f'score: {-1*best_grid_las.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "pipe = make_pipeline(ElasticNet())\n",
    "\n",
    "param_grid = {'elasticnet__alpha': [0.0001, 0.001, 0.01, 0.1, 1], # constant that multiplies the penalty terms, default = 1.0\n",
    "              'elasticnet__l1_ratio': [0.25, 0.5, 0.75]} # the mixing parameter, default = 0.5\n",
    "\n",
    "grid_en = GridSearchCV(pipe, param_grid = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n",
    "\n",
    "best_grid_en = grid_en.fit(X_train, y_train)\n",
    "\n",
    "en_param = best_grid_en.best_params_\n",
    "\n",
    "print(f'best_params_: {en_param}')\n",
    "print(f'score: {-1*best_grid_en.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "param_grid = {'bootstrap': [True, False],\n",
    "             'max_depth': [10, 25, 50],\n",
    "             'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_leaf': [1, 2, 3],\n",
    "             'min_samples_split': [2, 3],\n",
    "             'n_estimators': [300,500]\n",
    "             }\n",
    "\n",
    "grid_rf = RandomizedSearchCV(rf, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n",
    "\n",
    "best_grid_rf = grid_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_param = best_grid_rf.best_params_\n",
    "\n",
    "print(f'best_params_: {rf_param}')\n",
    "print(f'score: {-1*best_grid_rf.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "param_grid = {'loss': ['ls', 'huber'],  # loss function to be optimized (default = ’ls’)\n",
    "              'n_estimators': [200, 300],    # The number of boosting stages to perform (default = 100)\n",
    "              'learning_rate': [0.01, 0.1, 1],  # (default = 0.1)\n",
    "              'min_samples_split': [3, 5, 10],  # The minimum number of samples required to split an internal node (default = 2)\n",
    "              'max_depth': [3, 5, 10]}  # maximum depth of the individual regression estimators (default = 3)\n",
    "\n",
    "grid_gbr = RandomizedSearchCV(gbr, param_distributions = param_grid, cv = 5,scoring = 'neg_root_mean_squared_error', verbose = False, n_jobs = -1)\n",
    "\n",
    "best_grid_gbr = grid_gbr.fit(X_train, y_train)\n",
    "\n",
    "gbr_param = best_grid_gbr.best_params_\n",
    "\n",
    "print(f'best_params_: {gbr_param}')\n",
    "print(f'score: {-1*best_grid_gbr.best_score_:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_scores = cross_val_score(best_grid_en.best_estimator_, X_train, y_train,\n",
    "                             cv=5, n_jobs=-1, error_score=\"neg_root_mean_squared_error\")\n",
    "\n",
    "svr_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Algorithms <a id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Stacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. In this case we use XGBRegressor as meta-regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingCVRegressor(regressors = (best_grid_en.best_estimator_,\n",
    "                                          best_grid_rid.best_estimator_,\n",
    "                                          best_grid_las.best_estimator_,\n",
    "#                                           best_grid_rf.best_estimator_,\n",
    "                                          best_grid_gbr.best_estimator_,\n",
    "#                                           best_grid_xgb.best_estimator_,\n",
    "                                          best_grid_lgbm.best_estimator_,\n",
    "                                         best_grid_cat.best_estimator_),\n",
    "                            meta_regressor = best_grid_xgb.best_estimator_, \n",
    "                            use_features_in_secondary = True)\n",
    "\n",
    "\n",
    "stack.fit(X_train, y_train);\n",
    "\n",
    "stack_score = -cross_val_score(stack, X_train, y_train, scoring = 'neg_root_mean_squared_error', cv = 5)\n",
    "\n",
    "print(f'score: {stack_score.mean():.5f} ±{stack_score.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(\"Ridge\", best_grid_rid.best_estimator_),\n",
    "          (\"Lasso\", best_grid_las.best_estimator_),\n",
    "          (\"ElasticNet\", best_grid_en.best_estimator_),\n",
    "          ('RandomForest', best_grid_rf.best_estimator_),\n",
    "          ('GradientBoosting', best_grid_gbr.best_estimator_),\n",
    "          (\"XGBoost\", best_grid_xgb.best_estimator_),\n",
    "          (\"LightGBM\", best_grid_lgbm.best_estimator_),\n",
    "          (\"CatBoost\", best_grid_cat.best_estimator_)]\n",
    "\n",
    "print('='*30 +'RMSE TUNE MODEL' + '='*30)\n",
    "\n",
    "mean_rmse = []\n",
    "sd_rmse = []\n",
    "model_name = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    RMSE = -1*cross_val_score(model, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    print(f'{name:<25} {RMSE.mean():.5f} ±{RMSE.std():.3f}')\n",
    "    \n",
    "    mean_rmse.append(RMSE.mean())\n",
    "    sd_rmse.append(RMSE.std())\n",
    "    model_name.append(name)\n",
    "\n",
    "mean_rmse.append(stack_score.mean())\n",
    "sd_rmse.append(stack_score.std())\n",
    "model_name.append('Stacking')\n",
    "\n",
    "tune = pd.DataFrame({'model':model_name, 'mean_rmse':mean_rmse, 'sd_rmse':sd_rmse})\n",
    "\n",
    "\n",
    "sns.barplot(x=\"mean_rmse\", y=\"model\", data=tune, orient = 'h',**{'xerr': sd_rmse})\n",
    "plt.title('TUNE: Cross Validation Scores')\n",
    "plt.axvline(x = np.mean(mean_rmse), color = 'firebrick', linestyle = '--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Ridge', 'Lasso', 'Elastic_Net', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost', 'StackingRegressor']\n",
    "models = [best_grid_rid, best_grid_las, best_grid_en, best_grid_rf, best_grid_gbr, best_grid_xgb, best_grid_lgbm, best_grid_cat, stack]\n",
    "\n",
    "submission_ = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "for i in range(len(models)):\n",
    "    \n",
    "    y_pred = models[i].predict(X_test)\n",
    "    \n",
    "    y_pred_final = np.expm1(y_pred)\n",
    "    \n",
    "    submission_['SalePrice'] = y_pred_final\n",
    "    \n",
    "    submission_ = submission_[['Id', 'SalePrice']]\n",
    "    \n",
    "    submission_.to_csv('{}-{}.csv'.format(i+1, model_names[i]), index = False)\n",
    "\n",
    "print ('Submission files created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blending models by assigning weights:\n",
    "\n",
    "def blend_models_predict(X):\n",
    "    \n",
    "    return ((0.1 * best_grid_rid.predict(X)) +\n",
    "            (0.1 * best_grid_las.predict(X)) +\n",
    "            (0.1 * best_grid_en.predict(X)) +\n",
    "            (0.1 * best_grid_rf.predict(X)) +\n",
    "            (0.1 * best_grid_gbr.predict(X)) +\n",
    "            (0.1 * best_grid_xgb.predict(X)) +\n",
    "            (0.1 * best_grid_lgbm.predict(X)) +\n",
    "            (0.1 * best_grid_cat.predict(X)) +\n",
    "            (0.2 * stack.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "\n",
    "# Inversing and flooring log scaled sale price predictions\n",
    "submission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n",
    "\n",
    "submission = submission[['Id', 'SalePrice']]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('mysubmission7.csv', index=False)\n",
    "print('Save submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
