{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a classifier in machine learning that allows us to make predictions based on previous data. They are like a series of sequential “if … then” statements you feed new data into to get a result.\n",
    "\n",
    "Decision trees are easily created, visualized, and interpreted. Because of this, they are typically the first method used to model a dataset. The hierarchical structure and categorical nature of a decision tree makes it highly intuitive to implement.\n",
    "\n",
    "## Example\n",
    "To demonstrate decision trees, let’s take a look at an example. Imagine we want to predict whether Mike is going to go grocery shopping on any given day. We can look at previous factors that led Mike to go to the store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/example.png\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first split\n",
    "Here we can see the amount of grocery supplies Mike had, the weather, and whether Mike worked each day. Green rows are days he went to the store, and red days are those he didn’t. The goal of a decision tree is to try to understand why Mike goes to the store, and apply that to new data later on.\n",
    "\n",
    "Let’s divide the first attribute up into a tree. Mike can either have a low, medium, or high amount of supplies:\n",
    "\n",
    "<img src=\"image/decision_tree_1.png\" width=\"550\">\n",
    "\n",
    "\n",
    "Here we can see that Mike never goes to the store if he has a high amount of supplies. This is called a pure subset, a subset with only positive or only negative examples.With decision trees, there is no need to break a pure subset down further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our second split\n",
    "\n",
    "Let’s break the Med Supplies category into whether Mike worked that day:\n",
    "\n",
    "<img src=\"image/decision_tree_2.png\" width=\"550\">\n",
    "\n",
    "\n",
    "Here we can see we have two more pure subsets, so this tree is complete. We can replace any pure subsets with their respective answer - in this case, yes or no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our third split\n",
    "\n",
    "Finally, let’s split the Low Supplies category by the Weather attribute:\n",
    "    \n",
    "<img src=\"image/decision_tree_3.png\" width=\"650\">\n",
    "\n",
    "\n",
    "**Let's predict for this example : ['low',  'raining', 'yes']**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "Decision tree algorithms are also known as CART, or Classification and Regression Trees. A Classification Tree, like the one shown above, is used to get a result from a set of possible values. A Regression Tree is a decision tree where the result is a continuous value, such as the price of a car.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting (Induction)\n",
    "\n",
    "Decision trees are created through a process of splitting called induction, but how do we know when to split? We need a recursive algorithm that determines the best attributes to split on. One such algorithm is the greedy algorithm:\n",
    "\n",
    "1. Starting from the root, we create a split for each attribute.\n",
    "2. For each created split, calculate the cost of the split.\n",
    "3. Choose the split that costs the least.\n",
    "4. Recurse into the sub-trees and continue from step 1.\n",
    "\n",
    "This process is repeated until all nodes have the same value as the target result, or splitting adds no value to a prediction. This algorithm has the root node as the best classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost of Splitting\n",
    "The cost of a split is determined by a cost function. The goal of using a cost function is to split the data in a way that can be computed and that provides the most information gain.\n",
    "\n",
    "For classification trees, those that provide an answer rather than a value, we can compute imformation gain using Gini Impurities:\n",
    "\n",
    "#### Gini Impurity Function:\n",
    "\n",
    "<img src='image/Gini_impurity.png' width='120'>\n",
    "\n",
    "#### Gini Information Gain Formula:\n",
    "\n",
    "<img src='image/Gini_Information_Gain.png' width='250'>\n",
    "\n",
    "To calculate information gain, we first start by computing the Gini Impurity of our root node. Let's take a look at the data we used earlier:\n",
    "\n",
    "<img src=\"image/example.png\" width=\"350\">\n",
    "\n",
    "\n",
    "**Our root node is the target variable (shopped).** To calculate its Gini Impurity, we need to find the sum of probabilities squared for each outcome and subtract this result from one:\n",
    "\n",
    "<img src=\"image/Gini_2.png\" width=\"450\">\n",
    "\n",
    "Let's calculate the Gini Information Gain if we split on the first attribute, Supplies. We have three different categories we can split by - Low, Med, and High. For each of these, we calculate its Gini Impurity:\n",
    "\n",
    "<img src=\"image/Gini_4.png\" width=\"450\">\n",
    "\n",
    "As you can see, the impurity for High supplies is 0. This means that if we split on Supplies and receive High input, we immediately know what the outcome will be. To determine the Gini Information Gain for this split, we compute the root's impurity minus the weighted average of each child's impurity:\n",
    "\n",
    "<img src=\"image/Gini_7.png\" width=\"550\">\n",
    "\n",
    "We continue this pattern for every possible split, then choose the split that gives us the highest information gain value. Maximizing information gain leaves us with the most polarized splits possible, lowering the probability new input is incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import itertools\n",
    "import random \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The possible values for each class \n",
    "classes = {\n",
    "    'supplies': ['low', 'med', 'high'],\n",
    "    'weather':  ['raining', 'cloudy', 'sunny'],\n",
    "    'worked?':  ['yes', 'no']\n",
    "}\n",
    "\n",
    "# Our example data from the documentation\n",
    "data = [\n",
    "    ['low',  'sunny',   'yes'],\n",
    "    ['high', 'sunny',   'yes'],\n",
    "    ['med',  'cloudy',  'yes'],\n",
    "    ['low',  'raining', 'yes'],\n",
    "    ['low',  'cloudy',  'no' ],\n",
    "    ['high', 'sunny',   'no' ],\n",
    "    ['high', 'raining', 'no' ],\n",
    "    ['med',  'cloudy',  'yes'],\n",
    "    ['low',  'raining', 'yes'],\n",
    "    ['low',  'raining', 'no' ],\n",
    "    ['med',  'sunny',   'no' ],\n",
    "    ['high', 'sunny',   'yes']\n",
    "]\n",
    "\n",
    "# Our target variable, whether someone went shopping\n",
    "target = ['yes', 'no', 'no', 'no', 'yes', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [classes['supplies'], classes['weather'], classes['worked?']]\n",
    "encoder = OneHotEncoder(categories=categories)\n",
    "# encoder\n",
    "x_data = encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form and fit our decision tree to the now-encoded data\n",
    "classifier = DecisionTreeClassifier()\n",
    "tree = classifier.fit(x_data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes' 'no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'yes' 'yes' 'no']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use our tree to predict the outcome of the random values\n",
    "prediction_results = tree.predict(encoder.transform(data))\n",
    "\n",
    "print(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (\n",
    "    ['supplies=' + x for x in classes[\"supplies\"]] +\n",
    "    ['weather=' + x for x in classes[\"weather\"]] +\n",
    "    ['worked=' + x for x in classes[\"worked?\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows a visualization of the decision tree using graphviz\n",
    "# Note that sklearn is unable to generate non-binary trees, so these are based on individual options in each class\n",
    "dot_data = export_graphviz(tree, filled=True, proportion=True, feature_names=feature_names) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(filename='decision_tree', cleanup=True, view=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
